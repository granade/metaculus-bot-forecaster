{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/granade/metaculus-bot-forecaster/blob/main/granade_bot_v14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Forecasting Bot Template\n",
        "\n",
        "\n",
        "This is a simple bot template that you can use to forecast in the Metaculus AI Benchmarking Warmup Contest. It is a single shot GPT prompt that you are encouraged to experiment with!\n",
        "\n",
        "In order to run this notebook as is, you'll need to enter a few API keys (use the key icon on the left to input them):\n",
        "\n",
        "- `METACULUS_TOKEN`: you can find your Metaculus token under your bot's user settings page: https://www.metaculus.com/accounts/settings/, or on the bot registration page where you created the account: https://www.metaculus.com/aib/\n",
        "- `OPENAPI_API_KEY`: get one from OpenAIs page: https://platform.openai.com/settings/profile?tab=api-keys\n",
        "- `PERPLEXITY_API_KEY` - used to search up-to-date information about the question. Get one from https://www.perplexity.ai/settings/api\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-9qnfxffRmm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install -U -q pydrive2\n",
        "!pip install -qU openai asknews\n",
        "!pip install tiktoken\n",
        "!pip install anthropic==0.3.10\n",
        "!pip install lxml_html_clean\n",
        "\n",
        "# Import necessary libraries\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import csv\n",
        "import anthropic\n",
        "from asknews_sdk import AskNewsSDK\n",
        "from openai import OpenAI\n",
        "from datetime import timedelta\n",
        "import requests\n",
        "import tiktoken\n",
        "import re\n",
        "\n",
        "# use the below to detect if it's being run in google colab\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        from pydrive2.auth import GoogleAuth\n",
        "        from pydrive2.drive import GoogleDrive\n",
        "        from google.colab import auth\n",
        "        from google.colab import drive\n",
        "        from oauth2client.client import GoogleCredentials\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def load_secrets(secrets_path):\n",
        "    try:\n",
        "        with open(secrets_path, 'r') as secrets_file:\n",
        "            secrets = json.loads(secrets_file.read())\n",
        "            for k, v in secrets.items():\n",
        "                os.environ[k] = v\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading secrets from {secrets_path}: {e}\")\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    if 'secretsPath' in globals():\n",
        "        print(f\"secretsPath exists: {secretsPath}\")\n",
        "        load_secrets(secretsPath)\n",
        "        METACULUS_TOKEN = os.environ('METACULUS_TOKEN')\n",
        "        OPENAI_API_KEY = os.environ('OPENAI_API_KEY')\n",
        "        PERPLEXITY_API_KEY = os.environ('PERPLEXITY_API_KEY')\n",
        "        ASKNEWS_CLIENT_ID = os.environ('ASKNEWS_CLIENT_ID')\n",
        "        ASKNEWS_SECRET = os.environ('ASKNEWS_SECRET')\n",
        "        CLAUDE_API_KEY = os.environ('CLAUDE_API_KEY')\n",
        "        # And Other Keys Saved In GitHub Secrets\n",
        "    else:\n",
        "        raise NameError(\"secretsPath not defined\")\n",
        "except NameError:\n",
        "    print(\"Loading secrets from userdata (Google Colab)\")\n",
        "    METACULUS_TOKEN = userdata.get('METACULUS_TOKEN')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    PERPLEXITY_API_KEY = userdata.get('PERPLEXITY_API_KEY')\n",
        "    ASKNEWS_CLIENT_ID = userdata.get('ASKNEWS_CLIENT_ID')\n",
        "    ASKNEWS_SECRET = userdata.get('ASKNEWS_SECRET')\n",
        "    CLAUDE_API_KEY = userdata.get('CLAUDE_API_KEY')\n",
        "except KeyError as e:\n",
        "    print(f\"Missing required environment variable: {e}\")\n",
        "\n",
        "AUTH_HEADERS = {\"headers\": {\"Authorization\": f\"Token {METACULUS_TOKEN}\"}}\n",
        "API_BASE_URL = \"https://www.metaculus.com/api2\"\n",
        "WARMUP_TOURNAMENT_ID = 3294\n",
        "#3294 --> AI Test Bot Page\n",
        "#3349 --> AI Bot Competition\n",
        "#3366 --> Regular Quarterly Tournement\n",
        "SUBMIT_PREDICTION = False"
      ],
      "metadata": {
        "id": "MJJ_BYcHbVkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda8b594-a057-4dbb-a6ad-2a548aef87ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: anthropic==0.3.10 in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.10) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic==0.3.10) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.10) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.10) (2.9.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.10) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.10) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.10) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.10) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.10) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic==0.3.10) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic==0.3.10) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic==0.3.10) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic==0.3.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic==0.3.10) (2.23.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic==0.3.10) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.3.10) (2.0.7)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (4.9.4)\n",
            "Downloading lxml_html_clean-0.2.2-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.2.2\n",
            "Loading secrets from userdata (Google Colab)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT Prompt\n",
        "\n",
        "You can change the prompt below to experiment. Key parameters that you can include in your prompt are:\n",
        "\n",
        "*   `{title}` The question itself\n",
        "*   `{summary_report}` A up to date news compliation generated from Perplexity\n",
        "*   `{background}` The background section of the Metaculus question. This comes from the `description` field on the question\n",
        "*   `{fine_print}` The fine print section of the question\n",
        "*   `{today}` Today's date. Remember that your bot doesn't know the date unless you tell it explicitly!\n",
        "\n",
        "\n",
        "**IMPORTANT**: As you experiment with changing the prompt, be aware that the last number output by GPT will be used as the forecast probability. The last line in the template specifies that.\n"
      ],
      "metadata": {
        "id": "K1v6Sy5K-NJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ASKNEWS_CLEANUP_PROMPT = \"\"\"\n",
        "You are a professional researcher, who works for a forecaster.  The forecaster is workking\n",
        "on the following question:\n",
        "{title}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "As part of the forecasting process, the researcher pulled ~50 articles from a database\n",
        "that contains newspapers and other media articles from the around the world.\n",
        "\n",
        "The database is good at pulling lots of articles but not at indentifying the most important articles.\n",
        "That's where you come in.\n",
        "\n",
        "I need you to read through all the articles and send back the\n",
        "relevant ones given the question your client (the professional forecaster) is trying to answer.\n",
        "Your answer could be anywhere from 0 to 50 articles.\n",
        "\n",
        "Here is the stack of articles:\n",
        "{llm_context}\n",
        "\n",
        "Your final response should present the relevant articles in a format similar to the\n",
        "format you received them. Please present the title, summary, source, published date.\n",
        "There is no need to include the other fields in your final response.\n",
        "\n",
        "A few things to think about in your process.  First, you should weigh the quality of the sources.\n",
        "Some media is better than others so be sure to pull from higher quality sources.\n",
        "Second, try to get a wide-variety of perspectives.  Divergent views are useful -- even\n",
        "helpful.  Third, you should not develop a forecast; you should present the relevant articles.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "ASKNEWS_HOTNEWS_CLEANUP_PROMPT = \"\"\"\n",
        "You are a professional researcher, who works for a forecaster.  The forecaster is workking\n",
        "on the following question:\n",
        "{title}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "As part of the forecasting process, the researcher pulled ~10 articles from a media database that contains newspapers and other media articles from the around the world.\n",
        "All the articles are from the past 48 hours.  The idea is to get the latest news related to the topic so he or\n",
        "she is fully up-to-date.\n",
        "\n",
        "The database is good at pulling lots of articles but not at indentifying the most important articles.\n",
        "That's where you come in.\n",
        "\n",
        "I need you to read through all the articles and send back the\n",
        "relevant ones given the question your client (the professional forecaster) is trying to answer.\n",
        "\n",
        "You should be an emphasize on breaking news.  What is the latest news related to this question.\n",
        "\n",
        "Your answer could be anywhere from 0 to 10 articles.\n",
        "\n",
        "Here is the stack of articles:\n",
        "{hotnews}\n",
        "\n",
        "Your final response should present the relevant articles in a format similar to the\n",
        "format you received them. Please present the title, summary, source, published date.\n",
        "There is no need to include the other fields in your final response.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a professional forecaster, and I need your help making a prediction.\n",
        "\n",
        "Your goal is to make an accurate prediction. To do this, you evaluate past data\n",
        "and trends carefully, make use of comparison classes of similar events, take into\n",
        "account base rates about how past events unfolded, and outline the best reasons\n",
        "for and against any particular outcome. You know that great forecasters don't\n",
        "just forecast according to the \"vibe\" of the question -- they do the work.\n",
        "They think about the question in a structured way, recording their\n",
        "reasoning as they go, and they always consider multiple perspectives. You do not\n",
        "need to hedge your uncertainty, you are simply trying to give the most accurate\n",
        "probability.  Your answer will be evaluated later when the event unfolds.\n",
        "\n",
        "Here is some information about the question.\n",
        "\n",
        "The question is:\n",
        "{title}\n",
        "\n",
        "Here is some background on the question:\n",
        "{background}\n",
        "\n",
        "Here is how the question gets resolved:\n",
        "{resolution_criteria}\n",
        "\n",
        "Here is the fine print on the question:\n",
        "{fine_print}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "Your research assistant did some work to establish a fact-base on the question\n",
        "you're trying to answer.  Specifically, you sent her some important questions and she\n",
        "did work to answer each of them.  Here's what she found:\n",
        "\n",
        "{summary_report}\n",
        "\n",
        "I also thought it would be useful for you to have some background articles and other reporting on the question.\n",
        "I asked another research assistant to pull some headlines and article summaries from\n",
        "a wide range of media sources related to the question you're trying to forecast.\n",
        "The database he used has articles from the last year.  Here they are:\n",
        "\n",
        "{llm_context_cleaned}\n",
        "\n",
        "I asked your research assistant also to pull a few headlines from the past\n",
        "48 hours related to the question you're trying to forecast. Here are the headlines and a\n",
        "summary of each article: {hotnews_cleaned}.\n",
        "\n",
        "Let's now go through some steps that good forecasters use to answer a question.\n",
        "I am going to layout some questions I would like you to answer and to think\n",
        "about before you give a probability. You should give an explicit answer to all\n",
        "of these questions -- and think about them carefully -- before you give your\n",
        "probablity. Showing your work will help you develop a better answer.\n",
        "\n",
        "1. Given the question above, please rephrase and expand the question to help\n",
        "you do a better job answering it.  Maintain all of the information in the\n",
        "original question.\n",
        "\n",
        "2. Think about the default resolution, meaning if the question resolved today how\n",
        "would it resolve.\n",
        "\n",
        "3.  The time element is always important in prediction.  So make sure you know\n",
        "today's date.  Then think about how much time is left until this question gets resolved.\n",
        "In other words, if nothing else changes, what is the resolution?\n",
        "\n",
        "4. Using your knowledge of the world and topic, as well as the information provided,\n",
        "including the information provided by your research assistant, list a few reasons\n",
        "why the answer might be NO.  Rate the strength of those reasons.\n",
        "\n",
        "5. Using your knowledge of the world and topic, as well as the information provided,\n",
        "including the information provided by your research assistant, list a few reasons\n",
        "why the answer might be YES.  Rate the strength of those reasons.\n",
        "\n",
        "6. Think about historical data and determine a base rate for the question.  Here's a\n",
        "good definition of base rate: a base rate is the fundamental likelihood of an event\n",
        "occurring based on historical data.\n",
        "\n",
        "7. Your research assistant created an important file of factual information, which\n",
        "I provided you above. Your other research assistant pulled headlines and artciles.\n",
        "How do these inputs change your thinking about the question, if at all?\n",
        "\n",
        "8. Another important thing to think about is recent developments -- essentionally\n",
        "break news. Above I gave you some few headlines from the past\n",
        "24 hours related to the question you're trying to forecast. Sometimes they will\n",
        "be very relevant; sometimes they will be completely irrelevant. It's your job to\n",
        "sort that out. How do these headlines impact your prediction, if at all?\n",
        "\n",
        "9. You know that the resolution criteria and fine print of a question often\n",
        "contain important edge cases that should be considered. Considering the resolution criterion and\n",
        "fine-print provided to you above, how do you think that impacts the probabilities of a given\n",
        "outcome here?\n",
        "\n",
        "10. Now aggregate your considerations.  Think like a superforecaster (e.g., Nate\n",
        "Silver, Phil Tetlock).  Based on everything you've learning in steps 1 through 9,\n",
        "give us your best answer.  You should aggregate your answer into a probability between 0% (very, very unlikely) and 100% (very, very likely).\n",
        "Be sure to answer the question as it is phrased -- i.e., provide a probability for\n",
        "the question you're trying to answer (not the inverse).\n",
        "\n",
        "***Do not predict NONE or NO.***\n",
        "You should always provide a number.  That said, the number can be very low or very high. Don't be\n",
        "afraid to go to the extremes if your analysis suggests so.\n",
        "\n",
        "Thanks for your help.\n",
        "\n",
        "Follow these steps when generating an output:\n",
        "\n",
        "1) **show your work** Provide your analysis based on each of the steps described above i.e., write out an answer to each step.\n",
        "Then given the question, all the material provided to you, and your step-by-step work\n",
        "provide your expert forecast on whether or not the resolution criteria will be achieved and your rationale.\n",
        "Overall \"show your work\" will be several paragpraphs long.  That's okay -- take your time and write out what you need to write out.\n",
        "2) **determine a forecast probability** Given the resolution criteria and your rationale, determine a the probability (likelihood) that the resolution criteria will be achieved, this is an integer between 1 and 100.\n",
        "\n",
        "Output your response in the following JSON structure:\n",
        "\n",
        "{{\n",
        "\"rationale\": \"string\",\n",
        "\"probability\": \"integer between 0 and 100\"\n",
        "}}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE2 = \"\"\"\n",
        "You are a professional forecaster. Your goal is to make an accurate prediction\n",
        "on an important question. To do this, you evaluate past data and trends, make\n",
        "use of comparison classes of similar events, take into account base rates about\n",
        "how past events unfolded, and outline the best reasons for and against any particular outcome.\n",
        "\n",
        "I am going to give you some information about the question I need you to forecast.\n",
        "\n",
        "Your question is:\n",
        "{title}\n",
        "\n",
        "background:\n",
        "{background}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "Before you do any forecasting, your research assistant is going to do some work\n",
        "to discover important background information for your forecast.  You need to give\n",
        "the research assistant guidance on what would be most helpful to you.  The more explicit\n",
        "you are about what you want / need to know, the more likely your assistant is to help you.\n",
        "\n",
        "Given the question you need to forecast and the background -- as well as everything\n",
        "you know about forecasting -- what are three or so questions your research assistant\n",
        "could help you with.  These questions should primarily be factual in nature -- things you can\n",
        "look up in news sources, encyclopedia's, presses releases, on the internet, etc. Please list them\n",
        "in order of importance.\n",
        "\n",
        "Your response should be in the form of instructions to your research assistant, which\n",
        "I will pass on directly.  You should begin with some context on what\n",
        "you are trying to do.  Then list the ~5 or so questions you want him or her to research.\n",
        "You also should point out a couple of best practices to the research assistant:\n",
        "First, the research assistant should use a wide range of high quality sources -- especially\n",
        "news sources.  Second, he or she should not develop their own forecast. You need the research assistant to develop\n",
        "a useful factbase; you will then do the forecasting.  Include all of this in your instructions.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE4 = \"\"\"\n",
        "You are a professional forecaster, and I need your help making a prediction.\n",
        "Your goal is to make an accurate prediction.\n",
        "\n",
        "Here is some information about the question I need you to predict.\n",
        "\n",
        "The question is:\n",
        "{title}\n",
        "\n",
        "Here is some background on the question:\n",
        "{background}\n",
        "\n",
        "Here is how the question gets resolved:\n",
        "{resolution_criteria}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "You know that examining the reasoning of other\n",
        "forecasters is an excellent way to improve your own forecast. Below I have provided the reasoning from several other forecasters who predicted on the same question.\n",
        "Examine their reasoning and use it to inform your own, using your expertise as a forecaster to assess which reasoning seems strongest and which seems flawed,\n",
        "as well as which reasoning seems to incorporate the most accurate information about base rates and historic reference classes. Construct your own reasoning and forecast,\n",
        "describing your reasoning step by step and incorporating the strongest arguments from the other forecasters in a way that improves your own reasoning. First produce a\n",
        "one sentence summary of the reasoning of each forecaster, then describe your forecast.\n",
        "\n",
        "You have a team of forecasters who work for you.  In each case, they went through\n",
        "a multi-step process to make a prediction. They wrote up their work at every step.\n",
        "They also included a brief summary of their logic.\n",
        "\n",
        "Here are the forecasters' predictions and logic:\n",
        "\n",
        "Forecaster 1:\n",
        "{forecaster1}\n",
        "\n",
        "Forecaster 2:\n",
        "{forecaster2}\n",
        "\n",
        "Forecaster 3:\n",
        "{forecaster3}\n",
        "\n",
        "Forecaster 4:\n",
        "{forecaster4}\n",
        "\n",
        "Forecaster 5:\n",
        "{forecaster5}\n",
        "\n",
        "Your job is to review their work and then develop your own prediction.\n",
        "\n",
        "Here is one thing to think about in particular -- if the other forecasters have scores that are less than 10% or more than 90%\n",
        "chances are they are suffering from a failure to extremize the forecast.  What does that mean?\n",
        "When you get near extremes -- like 0% and 100% -- forecasters tend to hedge.  They tend to add or subtract\n",
        "a few percentage points.  That's dangerous.  Tell us what you really think!\n",
        "\n",
        "Now aggregate your considerations.  Think like a superforecaster (e.g., Nate\n",
        "Silver, Phil Tetlock).  Based on everything you've learned above, give us your best answer.\n",
        "You should aggregate your answer into a probability between 0% (very, very unlikely) and 100% (very, very likely).\n",
        "Be sure to answer the question as it is phrased -- i.e., provide a probability for\n",
        "the question you're trying to answer (not the inverse).\n",
        "\n",
        "Do not predict NONE or NO. You should always provide a number.  That said, the number can be very low or very high. Don't be\n",
        "afraid to go to the extremes if your analysis suggests so.\n",
        "\n",
        "Thanks for your help.\n",
        "\n",
        "\n",
        "Follow these steps when generating output:\n",
        "\n",
        "1) **provide rationale** This should have three parts.  Part I: Start by offering a one sentence summary of each of the forecasts submitted to you.  Part II: Then comment overall on the best and worst arguments.\n",
        "Part III: Given the question and everything else submitted to you, provide your expert forecasting rationale behind whether or not the resolution criteria will be achieved.\n",
        "2) **determine a forecast probability** Given the resolution criteria and your rationale, determine a the probability (likelihood) that the resolution criteria will be achieved, this is an integer between 0 and 100.\n",
        "\n",
        "Output your response in the following JSON structure:\n",
        "\n",
        "{{\n",
        "\"rationale\": \"string\",\n",
        "\"probability\": \"integer between 0 and 100\"\n",
        "}}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "CLAUDE_PROMPT_TEMPLATE = \"\"\"\n",
        "\\n\\nHuman:\n",
        "You are a professional forecaster, and I need your help making a prediction.\n",
        "\n",
        "Your goal is to make an accurate prediction. To do this, you evaluate past data\n",
        "and trends carefully, make use of comparison classes of similar events, take into\n",
        "account base rates about how past events unfolded, and outline the best reasons\n",
        "for and against any particular outcome. You know that great forecasters don't\n",
        "just forecast according to the \"vibe\" of the question -- they do the work.\n",
        "They think about the question in a structured way, recording their\n",
        "reasoning as they go, and they always consider multiple perspectives. You do not\n",
        "need to hedge your uncertainty, you are simply trying to give the most accurate\n",
        "probability.  Your answer will be evaluated later when the event unfolds.\n",
        "\n",
        "Here is some information about the question.\n",
        "\n",
        "The question is:\n",
        "{title}\n",
        "\n",
        "Here is some background on the question:\n",
        "{background}\n",
        "\n",
        "Here is how the question gets resolved:\n",
        "{resolution_criteria}\n",
        "\n",
        "Here is the fine print on the question:\n",
        "{fine_print}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "Your research assistant did some work to establish a fact-base on the question\n",
        "you're trying to answer.  Specifically, you sent her some important questions and she\n",
        "did work to answer each of them.  Here's what she found:\n",
        "\n",
        "{summary_report}\n",
        "\n",
        "I also thought it would be useful for you to have some background articles and other reporting on the question.\n",
        "I asked another research assistant to pull some headlines and article summaries from\n",
        "a wide range of media sources related to the question you're trying to forecast.\n",
        "The database he used has articles from the last year.  Here they are:\n",
        "\n",
        "{llm_context_cleaned}\n",
        "\n",
        "I asked your research assistant also to pull a few headlines from the past\n",
        "48 hours related to the question you're trying to forecast. Here are the headlines and a\n",
        "summary of each article: {hotnews_cleaned}.\n",
        "\n",
        "Let's now go through some steps that good forecasters use to answer a question.\n",
        "I am going to layout some questions I would like you to answer and to think\n",
        "about before you give a probability. You should give an explicit answer to all\n",
        "of these questions -- and think about them carefully -- before you give your\n",
        "probablity. Showing your work will help you develop a better answer.\n",
        "\n",
        "When you state or restate the question in your output, please do not write \"%\" instead\n",
        "spell out \"percent\" ... except in your final answer when you should write \"%\" as instructed below.  Thanks.\n",
        "\n",
        "1. Given the question above, please rephrase and expand the question to help\n",
        "you do a better job answering it.  Maintain all of the information in the\n",
        "original question.\n",
        "\n",
        "2. Think about the default resolution, meaning if the question resolved today how\n",
        "would it resolve.\n",
        "\n",
        "3.  The time element is always important in prediction.  So make sure you know\n",
        "today's date.  Then think about how much time is left until this question gets resolved.\n",
        "In other words, if nothing else changes, what is the resolution?\n",
        "\n",
        "4. Using your knowledge of the world and topic, as well as the information provided,\n",
        "including the information provided by your research assistant, list a few reasons\n",
        "why the answer might be NO.  Rate the strength of those reasons.\n",
        "\n",
        "5. Using your knowledge of the world and topic, as well as the information provided,\n",
        "including the information provided by your research assistant, list a few reasons\n",
        "why the answer might be YES.  Rate the strength of those reasons.\n",
        "\n",
        "6. Think about historical data and determine a base rate for the question.  Here's a\n",
        "good definition of base rate: a base rate is the fundamental likelihood of an event\n",
        "occurring based on historical data.\n",
        "\n",
        "7. Your research assistant created an important file of factual information, which\n",
        "I provided you above. Your other research assistant pulled headlines and artciles.\n",
        "How do these inputs change your thinking about the question, if at all?\n",
        "\n",
        "8. Another important thing to think about is recent developments -- essentionally\n",
        "break news. Above I gave you some few headlines from the past\n",
        "24 hours related to the question you're trying to forecast. Sometimes they will\n",
        "be very relevant; sometimes they will be completely irrelevant. It's your job to\n",
        "sort that out. How do these headlines impact your prediction, if at all?\n",
        "\n",
        "9. You know that the resolution criteria and fine print of a question often\n",
        "contain important edge cases that should be considered. Considering the resolution criterion and\n",
        "fine-print provided to you above, how do you think that impacts the probabilities of a given\n",
        "outcome here?\n",
        "\n",
        "10. Now aggregate your considerations.  Think like a superforecaster (e.g., Nate\n",
        "Silver, Phil Tetlock).  Based on everything you've learning in steps 1 through 9,\n",
        "give us your best answer. You should write your answer as: \"Probability: ZZ%\", 0-100.\n",
        "\n",
        "A few critical things to remember:\n",
        "a) Do not predict NONE or NO.\n",
        "b) You should always provide a number.\n",
        "c) That said, the number can be very low or very high. Don't be\n",
        "afraid to go to the extremes if your analysis suggests so.\n",
        "d) Be sure to answer the question as it is phrased -- i.e., provide a probability for\n",
        "the question you're trying to answer (not the inverse).\n",
        "\n",
        "You should also provide a three or four sentence summary of why you think that probability is correct.\n",
        "\n",
        "Thanks for your help!\n",
        "\n",
        "\\n\\nAssistant:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "vNStT_eV8tLG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some setup code\n",
        "\n",
        "This section sets up some simple helper code you can use to get data about forecasting questions and to submit a prediction"
      ],
      "metadata": {
        "id": "iDukuXArbgdm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HifodCwcGU0j",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def write_to_txt(question_id, label, text_string):\n",
        "  today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "  file_name = f'{today}_{question_id}.txt'\n",
        "  file_path = f'/content/drive/My Drive/snapshots/{file_name}'\n",
        "\n",
        "  with open(file_path, 'a') as f:\n",
        "    f.write(f\"**{label}:**\\n {text_string}\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "def find_number_before_percent(s):\n",
        "    # Use a regular expression to find all numbers followed by a '%'\n",
        "    matches = re.findall(r'(\\d+)%', s)\n",
        "    if matches:\n",
        "        # Return the last number found before a '%'\n",
        "        return int(matches[-1])\n",
        "    else:\n",
        "        # Return None if no number found\n",
        "        return None\n",
        "\n",
        "def post_question_comment(question_id, comment_text):\n",
        "    \"\"\"\n",
        "    Post a comment on the question page as the bot user.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.post(\n",
        "        f\"{API_BASE_URL}/comments/\",\n",
        "        json={\n",
        "            \"comment_text\": comment_text,\n",
        "            \"submit_type\": \"N\",\n",
        "            \"include_latest_prediction\": True,\n",
        "            \"question\": question_id,\n",
        "        },\n",
        "        **AUTH_HEADERS,\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "\n",
        "def get_claude_prediction(question_details, llm_context_cleaned, hotnews_cleaned, summary_report):\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
        "\n",
        "    title = question_details[\"title\"]\n",
        "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
        "    background = question_details[\"description\"]\n",
        "    fine_print = question_details[\"fine_print\"]\n",
        "    llm_context_cleaned=llm_context_cleaned\n",
        "    hotnews_cleaned=hotnews_cleaned\n",
        "    summary_report=summary_report\n",
        "\n",
        "    prompt = CLAUDE_PROMPT_TEMPLATE.format(\n",
        "      title=title,\n",
        "      today=today,\n",
        "      resolution_criteria=resolution_criteria,\n",
        "      background=background,\n",
        "      fine_print=fine_print,\n",
        "      llm_context_cleaned=llm_context_cleaned,\n",
        "      hotnews_cleaned=hotnews_cleaned,\n",
        "      summary_report=summary_report\n",
        "    )\n",
        "\n",
        "#    print(prompt)\n",
        "\n",
        "    response = client.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        max_tokens_to_sample=1000,\n",
        "        prompt=prompt,\n",
        "    )\n",
        "\n",
        "#    print(response.completion)\n",
        "    probability_match = find_number_before_percent(response.completion)\n",
        "    probability = int(probability_match) # int(match.group(1))\n",
        "#    print(f\"The extracted probability is: {probability}%\")\n",
        "    return probability, response.completion\n",
        "\n",
        "def post_question_prediction(question_id, prediction_percentage):\n",
        "    \"\"\"\n",
        "    Post a prediction value (between 1 and 100) on the question.\n",
        "    \"\"\"\n",
        "    url = f\"{API_BASE_URL}/questions/{question_id}/predict/\"\n",
        "    response = requests.post(\n",
        "        url,\n",
        "        json={\"prediction\": float(prediction_percentage) / 100},\n",
        "        **AUTH_HEADERS,\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "#    print(\"The prediction percentage is:\", prediction_percentage)\n",
        "\n",
        "\n",
        "def get_question_details(question_id):\n",
        "    \"\"\"\n",
        "    Get all details about a specific question.\n",
        "    \"\"\"\n",
        "    url = f\"{API_BASE_URL}/questions/{question_id}/\"\n",
        "    response = requests.get(\n",
        "        url,\n",
        "        **AUTH_HEADERS,\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    return json.loads(response.content)\n",
        "\n",
        "def list_questions(tournament_id=WARMUP_TOURNAMENT_ID, offset=0, count=10):\n",
        "\n",
        "    url_qparams = {\n",
        "        \"limit\": count,\n",
        "        \"offset\": offset,\n",
        "        \"has_group\": \"false\",\n",
        "        \"order_by\": \"-activity\",\n",
        "        \"forecast_type\": \"binary\",\n",
        "        \"project\": tournament_id,\n",
        "        \"status\": \"open\",\n",
        "        \"type\": \"forecast\",\n",
        "        \"include_description\": \"true\",\n",
        "    }\n",
        "    url = f\"{API_BASE_URL}/questions/\"\n",
        "    response = requests.get(url, **AUTH_HEADERS, params=url_qparams)\n",
        "    response.raise_for_status()\n",
        "    data = json.loads(response.content)\n",
        "    return data\n",
        "\n",
        "def get_asknews_llmcontext(query):\n",
        "\n",
        "  ask = AskNewsSDK(\n",
        "      client_id=ASKNEWS_CLIENT_ID,\n",
        "      client_secret=ASKNEWS_SECRET,\n",
        "      scopes=[\"news\"]\n",
        "  )\n",
        "\n",
        "  historical_response = ask.news.search_news(\n",
        "      query=query,\n",
        "      n_articles=50,\n",
        "      return_type=\"string\",\n",
        "      historical=True,\n",
        "      method=\"both\",\n",
        "      diversify_sources=True,\n",
        "      strategy=\"default\",\n",
        "      provocative=\"low\",\n",
        "      hours_back=1400,\n",
        "  )\n",
        "\n",
        "  llm_context = historical_response.as_string\n",
        "  return llm_context\n",
        "\n",
        "def asknews_cleanup(llm_context, title):\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    llm_context = llm_context\n",
        "    title = title\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": ASKNEWS_CLEANUP_PROMPT.format(\n",
        "                llm_context=llm_context,\n",
        "                today=today,\n",
        "                title=title,\n",
        "            )\n",
        "        }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    llm_context_cleaned = chat_completion.choices[0].message.content\n",
        "    return llm_context_cleaned\n",
        "\n",
        "def get_asknews_hotnews(query):\n",
        "  \"\"\"\n",
        "  Use the AskNews `news` endpoint to get news context for your query.\n",
        "  The full API reference can be found here: https://docs.asknews.app/en/reference#get-/v1/news/search\n",
        "  \"\"\"\n",
        "  ask = AskNewsSDK(\n",
        "      client_id=ASKNEWS_CLIENT_ID,\n",
        "      client_secret=ASKNEWS_SECRET,\n",
        "      scopes=[\"news\"]\n",
        "  )\n",
        "\n",
        "  hotnews_response = ask.news.search_news(\n",
        "      query=query,\n",
        "      n_articles=10,\n",
        "      return_type=\"string\",\n",
        "      historical=False,\n",
        "      method=\"both\",\n",
        "      diversify_sources=False,\n",
        "      strategy=\"default\",\n",
        "      similarity_score_threshold=0.9,\n",
        "      provocative=\"low\",\n",
        "      hours_back=48\n",
        "  )\n",
        "\n",
        "  hotnews = hotnews_response.as_string\n",
        "  return hotnews\n",
        "\n",
        "def asknews_hotnews_cleanup(hotnews, title):\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    hotnews = hotnews\n",
        "    title = title\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": ASKNEWS_HOTNEWS_CLEANUP_PROMPT.format(\n",
        "                hotnews=hotnews,\n",
        "                today=today,\n",
        "                title=title,\n",
        "            )\n",
        "        }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    hotnews_cleaned = chat_completion.choices[0].message.content\n",
        "    return hotnews_cleaned\n",
        "\n",
        "def call_perplexity(query):\n",
        "\n",
        "    title = question_details[\"title\"]\n",
        "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
        "    background = question_details[\"description\"]\n",
        "    fine_print = question_details[\"fine_print\"]\n",
        "\n",
        "    gpt_question = get_gpt_questions(question_details)\n",
        "    write_to_txt(question_id, \"Question for Perplexity\", gpt_question)\n",
        "\n",
        "    url = \"https://api.perplexity.ai/chat/completions\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"content-type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"llama-3.1-sonar-large-128k-chat\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": gpt_question,\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    response = requests.post(url=url, json=payload, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    return content\n",
        "\n",
        "def get_gpt_questions(question_details):\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    title = question_details[\"title\"]\n",
        "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
        "    background = question_details[\"description\"]\n",
        "    fine_print = question_details[\"fine_print\"]\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": PROMPT_TEMPLATE2.format(\n",
        "                title=title,\n",
        "                today=today,\n",
        "                background=background,\n",
        "                fine_print=fine_print,\n",
        "            )\n",
        "        }\n",
        "        ]\n",
        "    )\n",
        "    gpt_questions = chat_completion.choices[0].message.content\n",
        "    return gpt_questions\n",
        "\n",
        "def get_gpt_prediction(question_details, llm_context_cleaned, hotnews_cleaned, summary_report):\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    title = question_details[\"title\"]\n",
        "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
        "    background = question_details[\"description\"]\n",
        "    fine_print = question_details[\"fine_print\"]\n",
        "\n",
        "    summary_report = summary_report\n",
        "\n",
        "    filled_prompt = PROMPT_TEMPLATE.format(\n",
        "      title=title,\n",
        "      llm_context_cleaned=llm_context_cleaned,\n",
        "      hotnews_cleaned=hotnews_cleaned,\n",
        "      summary_report=summary_report,\n",
        "      today=today,\n",
        "      resolution_criteria=resolution_criteria,\n",
        "      background=background,\n",
        "      fine_print=fine_print\n",
        "    )\n",
        "\n",
        "#    print(filled_prompt)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": PROMPT_TEMPLATE.format(\n",
        "                title=title,\n",
        "                llm_context_cleaned=llm_context_cleaned,\n",
        "                hotnews_cleaned=hotnews_cleaned,\n",
        "                summary_report=summary_report,\n",
        "                today=today,\n",
        "                resolution_criteria=resolution_criteria,\n",
        "                background=background,\n",
        "                fine_print=fine_print,\n",
        "            )\n",
        "        }\n",
        "        ],\n",
        "        response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "\n",
        "    gpt_text = chat_completion.choices[0].message.content\n",
        "\n",
        "    parsed_dict = json.loads(gpt_text)\n",
        "    probability = parsed_dict.get(\"probability\", None)\n",
        "    rationale = parsed_dict.get(\"rationale\", None)\n",
        "\n",
        "    return probability, rationale\n",
        "\n",
        "def get_gpt_finalenhancedprediction(question_details, all_forecasts):\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    title = question_details[\"title\"]\n",
        "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
        "    background = question_details[\"description\"]\n",
        "    fine_print = question_details[\"fine_print\"]\n",
        "    forecaster1 = all_forecasts[0][1]\n",
        "    forecaster2 = all_forecasts[1][1]\n",
        "    forecaster3 = all_forecasts[2][1]\n",
        "    forecaster4 = all_forecasts[3][1]\n",
        "    forecaster5 = all_forecasts[4][1]\n",
        "\n",
        "    filled_prompt = PROMPT_TEMPLATE4.format(\n",
        "      title=title,\n",
        "      llm_context=llm_context,\n",
        "      hotnews=hotnews,\n",
        "      forecaster1 = forecaster1,\n",
        "      forecaster2 = forecaster2,\n",
        "      forecaster3 = forecaster3,\n",
        "      forecaster4 = forecaster4,\n",
        "      forecaster5 = forecaster5,\n",
        "      today=today,\n",
        "      resolution_criteria=resolution_criteria,\n",
        "      background=background,\n",
        "      fine_print=fine_print,    )\n",
        "\n",
        "#    print(filled_prompt)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": PROMPT_TEMPLATE4.format(\n",
        "                title=title,\n",
        "                llm_context=llm_context,\n",
        "                hotnews=hotnews,\n",
        "                forecaster1 = forecaster1,\n",
        "                forecaster2 = forecaster2,\n",
        "                forecaster3 = forecaster3,\n",
        "                forecaster4 = forecaster4,\n",
        "                forecaster5 = forecaster5,\n",
        "                today=today,\n",
        "                resolution_criteria=resolution_criteria,\n",
        "                background=background,\n",
        "                fine_print=fine_print,\n",
        "            )\n",
        "        }\n",
        "        ],\n",
        "        response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "\n",
        "    gpt_text = chat_completion.choices[0].message.content\n",
        "\n",
        "    parsed_dict = json.loads(gpt_text)\n",
        "    probability = parsed_dict.get(\"probability\", None)\n",
        "    rationale = parsed_dict.get(\"rationale\", None)\n",
        "\n",
        "    return probability, rationale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT prediction and submitting a forecast\n",
        "\n",
        "This is an example of how you can use the helper functions from above."
      ],
      "metadata": {
        "id": "9WUvm1tVmMkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "questions = list_questions()\n",
        "\n",
        "open_questions_ids = []\n",
        "\n",
        "for question in questions[\"results\"]:\n",
        "  if question[\"active_state\"] == \"OPEN\":\n",
        "# and question['my_predictions'] is None and question[\"id\"] != 0:\n",
        "#    print(f\"ID: {question['id']}\\nQ: {question['title']}\\nCloses: {question['close_time']}\")\n",
        "    open_questions_ids.append(question[\"id\"])\n",
        "\n",
        "for id in open_questions_ids:\n",
        "\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#  question_id = 27911\n",
        "  question_id = id\n",
        "  question_details = get_question_details(question_id)\n",
        "\n",
        "  print(question_id)\n",
        "  print(\"Title: \", question_details['title'])\n",
        "\n",
        "  write_to_txt(question_id, \"Questinion ID\", str(question_id))\n",
        "  write_to_txt(question_id, \"Title\", question_details['title'])\n",
        "  write_to_txt(question_id, \"Resolution Criteria\", question_details['resolution_criteria'])\n",
        "  write_to_txt(question_id, \"Description\", question_details['description'])\n",
        "  write_to_txt(question_id, \"Fine Print\", question_details['fine_print'])\n",
        "\n",
        "\n",
        "  title = question_details[\"title\"]\n",
        "\n",
        "  llm_context = get_asknews_llmcontext(title)\n",
        "#  print(\"AskNews historical articles: \", llm_context)\n",
        "  write_to_txt(question_id, \"AskNews Background\", llm_context)\n",
        "\n",
        "  llm_context_cleaned = asknews_cleanup(llm_context, title)\n",
        "#  print(\"AskNews cleaned articles: \", llm_context_cleaned)\n",
        "  write_to_txt(question_id, \"AskNews Background Cleaned\", llm_context_cleaned)\n",
        "\n",
        "  hotnews = get_asknews_hotnews(title)\n",
        "#  print(\"AskNews hotnews: \", hotnews)\n",
        "  write_to_txt(question_id, \"AskNews Hotnews\", hotnews)\n",
        "\n",
        "  hotnews_cleaned = asknews_cleanup(hotnews, title)\n",
        "#  print(\"AskNews hotnews: \", hotnews_cleaned)\n",
        "  write_to_txt(question_id, \"AskNews Hotnews Cleaned\", hotnews_cleaned)\n",
        "\n",
        "  summary_report = call_perplexity(title)\n",
        "#  print(\"Perplixity said: \", summary_report)\n",
        "  write_to_txt(question_id, \"Perplixity Report\", summary_report)\n",
        "\n",
        "  all_forecasts = []\n",
        "  promptset = [1, 2, 3, 4, 5]\n",
        "\n",
        "  prompt = 0\n",
        "  while prompt < 5:\n",
        "      probability, rationale = get_gpt_prediction(question_details, llm_context_cleaned, hotnews_cleaned, summary_report)\n",
        "      print(\"OpenAI predicted: \", probability)\n",
        "#      print(\"GPT said: \", rationale)\n",
        "      write_to_txt(question_id, \"GPT Forecast\", rationale)\n",
        "      if probability == None:\n",
        "          prompt = prompt + 0\n",
        "      else:\n",
        "          all_forecasts.append((probability, rationale))\n",
        "          prompt = prompt + 1\n",
        "\n",
        "  prompt2 = 0\n",
        "  while prompt2 < 1:\n",
        "      probability, rationale = get_gpt_finalenhancedprediction(question_details, all_forecasts)\n",
        "      print(\"GPT integrated predicted: \", probability)\n",
        "#      print(\"GPT said: \", rationale)\n",
        "      write_to_txt(question_id, \"GPT Integrated Forecast\", rationale)\n",
        "      if probability == None:\n",
        "          prompt2 = prompt2 + 0\n",
        "      else:\n",
        "          all_forecasts.append((probability, rationale))\n",
        "          prompt2 = prompt2 + 1\n",
        "\n",
        "  forecaster_weight = 0.1\n",
        "  weighted_forecast = forecaster_weight*float(all_forecasts[0][0]) + forecaster_weight*float(all_forecasts[1][0]) + forecaster_weight*float(all_forecasts[2][0]) + forecaster_weight*float(all_forecasts[3][0]) + forecaster_weight*float(all_forecasts[4][0]) + 0.5*float(all_forecasts[5][0])\n",
        "  weighted_forecast = int(weighted_forecast)\n",
        "\n",
        "  print(\"Final forecast for submission is: \", weighted_forecast)\n",
        "  prediction = weighted_forecast\n",
        "  comment = (all_forecasts[5][1])\n",
        "\n",
        "  if prediction is not None and SUBMIT_PREDICTION:\n",
        "      post_question_prediction(question_id, prediction)\n",
        "      post_question_comment(question_id, comment)\n",
        "      print(\"The submitted predicition is: \", prediction)\n",
        "#      print(\"The submitted comment is: \", comment)\n",
        "      write_to_txt(question_id, \"Submitted forecast is\", prediction)\n",
        "      write_to_txt(question_id, \"GPT Integrated Forecast\", rationale)\n",
        "\n",
        "  new_array = []\n",
        "  new_array = np.insert(new_array, 0, question_id)\n",
        "  new_array = np.insert(new_array, 1, all_forecasts[0][0])\n",
        "  new_array = np.insert(new_array, 2, all_forecasts[1][0])\n",
        "  new_array = np.insert(new_array, 3, all_forecasts[2][0])\n",
        "  new_array = np.insert(new_array, 4, all_forecasts[3][0])\n",
        "  new_array = np.insert(new_array, 5, all_forecasts[4][0])\n",
        "  new_array = np.insert(new_array, 6, all_forecasts[5][0])\n",
        "  new_array = np.insert(new_array, 7, weighted_forecast)\n",
        "\n",
        "#  print(new_array)\n",
        "\n",
        "\n",
        "  file_path = '/content/drive/My Drive/data.csv'\n",
        "  with open(file_path, 'a', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      writer.writerow(new_array)\n",
        "\n",
        "  for prompt in promptset:\n",
        "      prediction, gpt_result = get_claude_prediction(question_details, llm_context_cleaned, hotnews_cleaned, summary_report)\n",
        "      all_forecasts.append((prediction, gpt_result))\n",
        "#      print(\"Claude said: \", gpt_result)\n",
        "      print(\"Claude predicted: \", prediction)\n",
        "      write_to_txt(question_id, \"Claude Forecast\", gpt_result)\n",
        "\n",
        "  new_array = np.insert(new_array, 8, all_forecasts[6][0])\n",
        "  new_array = np.insert(new_array, 9, all_forecasts[7][0])\n",
        "  new_array = np.insert(new_array, 10, all_forecasts[8][0])\n",
        "  new_array = np.insert(new_array, 11, all_forecasts[9][0])\n",
        "  new_array = np.insert(new_array, 12, all_forecasts[10][0])\n",
        "\n",
        "  for prompt in promptset:\n",
        "      probability, rationale = get_gpt_prediction(question_details, llm_context_cleaned, hotnews_cleaned, summary_report)\n",
        "      all_forecasts.append((probability, rationale))\n",
        "      print(\"OpenAI predicted: \", probability)\n",
        "#      print(\"GPT said: \", rationale)\n",
        "      write_to_txt(question_id, \"GPT Forecast\", rationale)\n",
        "\n",
        "  new_array = np.insert(new_array, 13, all_forecasts[11][0])\n",
        "  new_array = np.insert(new_array, 14, all_forecasts[12][0])\n",
        "  new_array = np.insert(new_array, 15, all_forecasts[13][0])\n",
        "  new_array = np.insert(new_array, 16, all_forecasts[14][0])\n",
        "  new_array = np.insert(new_array, 17, all_forecasts[15][0])\n",
        "\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  file_path = '/content/drive/My Drive/data.csv'\n",
        "  with open(file_path, 'a', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      writer.writerow(new_array)\n",
        "\n",
        "if in_colab():\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()\n",
        "else: # in github\n",
        "  import sys\n",
        "  sys.exit()\n"
      ],
      "metadata": {
        "id": "KbQ6dmk9gzfk",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "a809d5b6-1080-48dd-e6e8-5391624802bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "27890\n",
            "Title:  [PRACTICE] Will Donald Trump be elected US President in 2024?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3a949dd7ece8>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mwrite_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AskNews Background\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mllm_context_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masknews_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m#  print(\"AskNews cleaned articles: \", llm_context_cleaned)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mwrite_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AskNews Background Cleaned\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_context_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-6e220e789ffa>\u001b[0m in \u001b[0;36masknews_cleanup\u001b[0;34m(llm_context, title)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    702\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         )\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 937\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    974\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    955\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    114\u001b[0m                 trace.return_value = (\n\u001b[1;32m    115\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}